{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "INSTALL PREREQUISITES"
      ],
      "metadata": {
        "id": "6KLs2aImzW0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlxCfEHljWWz",
        "outputId": "9603a6a4-12ba-4131-cd35-58458b58cacc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIbJUp_js7eM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch as t\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import time\n",
        "import pickle\n",
        "import scipy.sparse as sp\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YcqPD0Js7an",
        "outputId": "faa094a8-53b4-4956-a8d5-a6be49e225e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA GPU is available\n"
          ]
        }
      ],
      "source": [
        "if t.cuda.is_available():\n",
        "    print(\"CUDA GPU is available\")\n",
        "else:\n",
        "    print(\"CUDA GPU is not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GET THE DATA FROM THE DATASET"
      ],
      "metadata": {
        "id": "sPXfdxzizcAQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlwLzRFIs7Xw",
        "outputId": "7df47df5-d086-40cb-b491-5dd569adb426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmQPUsaHs7Lx"
      },
      "outputs": [],
      "source": [
        "def read_item_index_to_entity_id_file():\n",
        "    file = '/content/drive/MyDrive/Code/CGAT-main/dataset/mus/item_index2entity_id.txt'\n",
        "    print('reading item index to entity id file: ' + file + ' ...')\n",
        "    i = 0\n",
        "    for line in open(file, encoding='utf-8').readlines():\n",
        "        item_index = line.strip().split('\\t')[0]\n",
        "        satori_id = line.strip().split('\\t')[1]\n",
        "        item_index_old2new[item_index] = i\n",
        "        entity_id2index[satori_id] = i\n",
        "        i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OarnRFscs7I5"
      },
      "outputs": [],
      "source": [
        "def convert_kg():\n",
        "    print('converting kg file ...')\n",
        "    entity_cnt = len(entity_id2index)\n",
        "    relation_cnt = 0\n",
        "\n",
        "    # writer = open('../data/' + DATASET +\n",
        "    #               '/kg_final.txt', 'w', encoding='utf-8')\n",
        "\n",
        "    files = []\n",
        "    files.append(open('/content/drive/MyDrive/Code/CGAT-main/dataset/mus/kg.txt', encoding='utf-8'))\n",
        "    kg_final = []\n",
        "\n",
        "    for file in files:\n",
        "        for line in file:\n",
        "            array = line.strip().split('\\t')\n",
        "            head_old = array[0]\n",
        "            relation_old = array[1]\n",
        "            tail_old = array[2]\n",
        "\n",
        "            if head_old not in entity_id2index:\n",
        "                entity_id2index[head_old] = entity_cnt\n",
        "                entity_cnt += 1\n",
        "            head = entity_id2index[head_old]\n",
        "\n",
        "            if tail_old not in entity_id2index:\n",
        "                entity_id2index[tail_old] = entity_cnt\n",
        "                entity_cnt += 1\n",
        "            tail = entity_id2index[tail_old]\n",
        "\n",
        "            if relation_old not in relation_id2index:\n",
        "                relation_id2index[relation_old] = relation_cnt\n",
        "                relation_cnt += 1\n",
        "            relation = relation_id2index[relation_old]\n",
        "\n",
        "            kg_final.append([head, relation, tail])\n",
        "        file.close()\n",
        "\n",
        "    np.save('/content/drive/MyDrive/Code/CGAT-main/dataset/final' + '/kg_final.npy', kg_final)\n",
        "\n",
        "    print('number of entities (containing items): %d' % entity_cnt)\n",
        "    print('number of relations: %d' % relation_cnt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVSis0khs7GM"
      },
      "outputs": [],
      "source": [
        "def convert_rating():\n",
        "    print('reading rating file ...')\n",
        "    file = '/content/drive/MyDrive/Code/CGAT-main/dataset/mus/user_artists.dat'\n",
        "    print(file)\n",
        "    user_pos_ratings = dict()\n",
        "\n",
        "    for line in open(file, encoding='utf-8').readlines()[1:]:\n",
        "        array = line.strip().split('\\t')\n",
        "\n",
        "        item_index_old = array[1]\n",
        "\n",
        "        if item_index_old not in item_index_old2new:  # the item is not in the final item set\n",
        "            continue\n",
        "        item_index = item_index_old2new[item_index_old]\n",
        "\n",
        "        user_index_old = int(array[0])\n",
        "\n",
        "        rating = float(array[2])\n",
        "        if rating >= 0:\n",
        "            if user_index_old not in user_pos_ratings:\n",
        "                user_pos_ratings[user_index_old] = set()\n",
        "            user_pos_ratings[user_index_old].add(item_index)\n",
        "    print('converting rating file ...')\n",
        "\n",
        "    user_cnt = 0\n",
        "    user_index_old2new = dict()\n",
        "    rating_all = []\n",
        "    # store the pairs of user and positive or negetive(unwatched) item\n",
        "    for user_index_old, pos_item_set in user_pos_ratings.items():\n",
        "        if user_index_old not in user_index_old2new:\n",
        "            user_index_old2new[user_index_old] = user_cnt\n",
        "            user_cnt += 1\n",
        "        user_index = user_index_old2new[user_index_old]\n",
        "        for item in pos_item_set:\n",
        "            rating_all.append([user_index, item])\n",
        "\n",
        "    np.save('/content/drive/MyDrive/Code/CGAT-main/dataset/final' + '/ratings_final.npy', rating_all)\n",
        "    item_set = set(item_index_old2new.values())\n",
        "    data_split(item_set)\n",
        "    print('number of users: %d' % user_cnt)\n",
        "    print('number of items: %d' % len(item_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZsKCiLcs7DU"
      },
      "outputs": [],
      "source": [
        "def data_split(item_set):\n",
        "    rating_file = '/content/drive/MyDrive/Code/CGAT-main/dataset/final/ratings_final.npy'\n",
        "    rating_np = np.load(rating_file)\n",
        "    user_positive_items = dict()\n",
        "    for rate in rating_np:\n",
        "        user = rate[0]\n",
        "        item = rate[1]\n",
        "        if user not in user_positive_items:\n",
        "            user_positive_items[user] = []\n",
        "        user_positive_items[user].append(item)\n",
        "    eval_ratio = 0.2\n",
        "    test_ratio = 0.2\n",
        "    n_ratings = rating_np.shape[0]\n",
        "\n",
        "    eval_indices = np.random.choice(n_ratings, size=int(\n",
        "        n_ratings * eval_ratio), replace=False)\n",
        "    left = set(range(n_ratings)) - set(eval_indices)\n",
        "    test_indices = np.random.choice(list(left), size=int(\n",
        "        n_ratings * test_ratio), replace=False)\n",
        "    train_indices = list(left - set(test_indices))\n",
        "    user_history_dict = dict()\n",
        "    for i in train_indices:\n",
        "        user = rating_np[i][0]\n",
        "        item = rating_np[i][1]\n",
        "        if user not in user_history_dict:\n",
        "            user_history_dict[user] = []\n",
        "        user_history_dict[user].append(item)\n",
        "    train_indices = [i for i in train_indices if rating_np[i]\n",
        "                     [0] in user_history_dict]\n",
        "    eval_indices = [i for i in eval_indices if rating_np[i]\n",
        "                    [0] in user_history_dict]\n",
        "    test_indices = [i for i in test_indices if rating_np[i]\n",
        "                    [0] in user_history_dict]\n",
        "    train_data = rating_np[train_indices]\n",
        "    eval_data = rating_np[eval_indices]\n",
        "    test_data = rating_np[test_indices]\n",
        "\n",
        "    np.save('/content/drive/MyDrive/Code/CGAT-main/dataset/final'+ '/user_history_dict.npy', user_history_dict)\n",
        "\n",
        "    negative_sample('train', train_data, user_history_dict,\n",
        "                    user_positive_items, item_set, ratio=5)\n",
        "    negative_sample('eval', eval_data, user_history_dict,\n",
        "                    user_positive_items, item_set)\n",
        "    negative_sample('test', test_data, user_history_dict,\n",
        "                    user_positive_items, item_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FkVoun1s7Al"
      },
      "outputs": [],
      "source": [
        "def negative_sample(datype, data, user_history_dict, user_positive_ratings, item_set, ratio=1):\n",
        "    if datype == 'train':\n",
        "        history = user_history_dict\n",
        "    else:\n",
        "        history = user_positive_ratings\n",
        "    split_data = []\n",
        "    for i in data:\n",
        "        user_index = i[0]\n",
        "        pos_item_index = i[1]\n",
        "        negative_set = item_set - set(history[user_index])\n",
        "        for neg_item_index in np.random.choice(list(negative_set), size=ratio, replace=False):\n",
        "            split_data.append([user_index, pos_item_index, neg_item_index])\n",
        "    np.save('/content/drive/MyDrive/Code/CGAT-main/dataset/final'\n",
        "            '/' + datype + '_data.npy', split_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVtMLDHQs7SH",
        "outputId": "89f3baa3-7e2e-4971-bb0d-9254363a0471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading item index to entity id file: /content/drive/MyDrive/Code/CGAT-main/dataset/mus/item_index2entity_id.txt ...\n",
            "converting kg file ...\n",
            "number of entities (containing items): 9366\n",
            "number of relations: 60\n",
            "reading rating file ...\n",
            "/content/drive/MyDrive/Code/CGAT-main/dataset/mus/user_artists.dat\n",
            "converting rating file ...\n",
            "number of users: 1872\n",
            "number of items: 3846\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    np.random.seed(555)\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-d', '--dataset', type=str,\n",
        "                        default='mus', help='which dataset to preprocess')\n",
        "    parser.add_argument('-f')\n",
        "    args = parser.parse_args()\n",
        "    DATASET = args.dataset\n",
        "    entity_id2index = dict()\n",
        "    relation_id2index = dict()\n",
        "    item_index_old2new = dict()\n",
        "    read_item_index_to_entity_id_file()\n",
        "    convert_kg()\n",
        "    convert_rating()\n",
        "    print('done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ygtye2us69e"
      },
      "outputs": [],
      "source": [
        "adj_entity_gb_p = 'adj_entity_gb'\n",
        "def load_data(args):\n",
        "    train_data, eval_data, test_data, user_history_dict, n_user, n_item = load_rating(args)\n",
        "    print(args)\n",
        "    print(n_item)\n",
        "    n_entity, n_relation, kg, adj_enlc, adj_relc, adj_engb = load_kg(args, n_item)\n",
        "    user_history_dict = fix_userhist(args, user_history_dict)\n",
        "    print('Loaded data')\n",
        "    return train_data, eval_data, test_data, n_entity, n_relation, n_user, n_item, kg, adj_enlc, adj_relc, user_history_dict, adj_engb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Brl9W0rs662"
      },
      "outputs": [],
      "source": [
        "def load_rating(args):\n",
        "    print('reading rating file ...')\n",
        "\n",
        "    # reading rating file\n",
        "    train_file = '/content/drive/MyDrive/Code/CGAT-main/dataset/final/train_data.npy'\n",
        "    eval_file = '/content/drive/MyDrive/Code/CGAT-main/dataset/final/eval_data.npy'\n",
        "    test_file = '/content/drive/MyDrive/Code/CGAT-main/dataset/final/test_data.npy'\n",
        "    train_np = np.load(train_file)\n",
        "    test_np = np.load(test_file)\n",
        "    eval_np = np.load(eval_file)\n",
        "\n",
        "    user_history_dict = dict()\n",
        "    for i in train_np:\n",
        "        user = i[0]\n",
        "        item = i[1]\n",
        "        if user not in user_history_dict:\n",
        "            user_history_dict[user] = set()\n",
        "        user_history_dict[user].add(item)\n",
        "    for user, history_item in user_history_dict.items():\n",
        "        user_history_dict[user] = list(history_item)\n",
        "\n",
        "    n_user = max(set(train_np[:, 0]) | set(\n",
        "        test_np[:, 0]) | set(eval_np[:, 0])) + 1\n",
        "    n_item = max(set(train_np[:, 1]) | set(train_np[:, 2]) | set(\n",
        "        test_np[:, 1]) | set(test_np[:, 2]) | set(eval_np[:, 1]) | set(eval_np[:, 2])) + 1\n",
        "    interaction = int(len(train_np) / 5) + len(test_np) + len(eval_np)\n",
        "\n",
        "    print('n_user=%d, n_item=%d' % (n_user, n_item))\n",
        "    print('n_interaction=%d' % interaction)\n",
        "    return train_np, eval_np, test_np, user_history_dict, n_user, n_item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a30T8sgfO63H"
      },
      "outputs": [],
      "source": [
        "def load_kg(args, n_item):\n",
        "    print('reading KG file ...')\n",
        "    # reading kg file\n",
        "    kg_file = '/content/drive/MyDrive/Code/CGAT-main/dataset/final/kg_final'\n",
        "    if os.path.exists(kg_file + '.npy'):\n",
        "        kg_np = np.load(kg_file + '.npy')\n",
        "    else:\n",
        "        kg_np = np.loadtxt(kg_file + '.txt', dtype=np.int32)\n",
        "\n",
        "    n_entity = len(set(kg_np[:, 0]) | set(kg_np[:, 2]))\n",
        "    n_relation = len(set(kg_np[:, 1]))\n",
        "    kg_entity, kg_relation, adj_sp = construct_kg(\n",
        "        args, kg_np, n_relation, n_entity)\n",
        "    adj_entity, adj_relation = construct_adj(\n",
        "        args, kg_entity, kg_relation, n_item)\n",
        "    adj_enbro = construct_adj_gb(\n",
        "        args, kg_entity, n_item, adj_sp)\n",
        "    kg_train = '/content/drive/MyDrive/Code/CGAT-main/dataset/' + args.dataset + '/kg_train'\n",
        "    if os.path.exists(kg_train + '.npy'):\n",
        "        kg_np_ne = np.load(kg_train + '.npy')\n",
        "    else:\n",
        "        tail_entity = kg_np[:, 2]\n",
        "        neg_entity = np.array([np.random.choice(np.delete(np.arange(\n",
        "            n_entity), tail_entity[i], axis=0), 1) for i in range(len(tail_entity))])  # xiugai\n",
        "        kg_np_ne = np.concatenate((kg_np, neg_entity), 1)\n",
        "        np.save(kg_train + '.npy', kg_np_ne)\n",
        "    print('number of entity ', n_entity)\n",
        "    print('number of relations', n_relation)\n",
        "    print('number of triples', len(kg_np))\n",
        "    return n_entity, n_relation, kg_np_ne, adj_entity, adj_relation, adj_enbro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WevqoCJws607"
      },
      "outputs": [],
      "source": [
        "def construct_kg(args, kg_np, num_relation, n_entity):\n",
        "    print('constructing knowledge graph...')\n",
        "    kg_entity = dict()\n",
        "    kg_relation = dict()\n",
        "    for head, relation, tail in kg_np:\n",
        "        if head not in kg_entity:\n",
        "            kg_entity[head] = []\n",
        "            kg_relation[head] = []\n",
        "        kg_entity[head].append(tail)\n",
        "        kg_relation[head].append(relation)\n",
        "        if tail not in kg_entity:\n",
        "            kg_entity[tail] = []\n",
        "            kg_relation[tail] = []\n",
        "        kg_entity[tail].append(head)\n",
        "        kg_relation[tail].append(relation + num_relation)\n",
        "    a_rows = kg_np[:, 0]\n",
        "    a_cols = kg_np[:, 2]\n",
        "    a_vals = [1] * len(a_rows)\n",
        "    c_vals = [1] * n_entity\n",
        "    c_rows = [i for i in range(n_entity)]\n",
        "    c_cols = c_rows\n",
        "    a_adj = sp.coo_matrix((a_vals, (a_rows, a_cols)), shape=(n_entity, n_entity))\n",
        "    b_adj = sp.coo_matrix((a_vals, (a_cols, a_rows)),shape=(n_entity, n_entity))\n",
        "    c_adj = sp.coo_matrix((c_vals, (c_rows, c_cols)),shape=(n_entity, n_entity))\n",
        "    adj = (a_adj + b_adj + c_adj).tolil()\n",
        "\n",
        "    return kg_entity, kg_relation, adj\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M60EUyJys6yE"
      },
      "outputs": [],
      "source": [
        "def construct_adj(args, kg_entity, kg_relation, item_num):\n",
        "    # each line of adj_entity stores the sampled neighbor entities for a given entity\n",
        "    # each line of adj_relation stores the corresponding sampled neighbor relations\n",
        "    print('constructing local neighbor entities of items...')\n",
        "    adj_entity = np.zeros([item_num, args.n_neighbor], dtype=np.int64)\n",
        "    adj_relation = np.zeros([item_num, args.n_neighbor], dtype=np.int64)\n",
        "    for entity in range(item_num):\n",
        "        neighbors = kg_entity[entity]\n",
        "        n_neighbors = len(neighbors)\n",
        "        if n_neighbors >= args.n_neighbor:\n",
        "            sampled_indices = np.random.choice(\n",
        "                list(range(n_neighbors)), size=args.n_neighbor, replace=False)\n",
        "        else:\n",
        "            sampled_indices = np.random.choice(\n",
        "                list(range(n_neighbors)), size=args.n_neighbor, replace=True)\n",
        "        adj_entity[entity] = np.array(\n",
        "            [kg_entity[entity][i] for i in sampled_indices])\n",
        "        adj_relation[entity] = np.array(\n",
        "            [kg_relation[entity][i] for i in sampled_indices])\n",
        "\n",
        "    return adj_entity, adj_relation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qxkDv_ks6vN"
      },
      "outputs": [],
      "source": [
        "def construct_adj_gb(args, kg_entity, item_num, adj_sp):\n",
        "    print('constructing global neighbor entities of items...')\n",
        "    file = '/content/drive/MyDrive/Code/CGAT-main/dataset/' + args.dataset + '/' + adj_entity_gb_p\n",
        "    if os.path.exists(file + '.npy'):\n",
        "        adj_entity_gb = np.load(file + '.npy')\n",
        "        adj_entity_gb = adj_entity_gb[:, -args.n_neighbor:]\n",
        "    else:\n",
        "        number = 50\n",
        "        adj_entity_gb = np.zeros([item_num, number], dtype=np.int64)\n",
        "        for item in range(item_num):\n",
        "            adj_entity_gb[item] = depth_search(\n",
        "                args, kg_entity, item, adj_sp, number)\n",
        "        np.save(file + '.npy', adj_entity_gb)\n",
        "        adj_entity_gb = adj_entity_gb[:, -args.n_neighbor:]\n",
        "    return adj_entity_gb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCksfTAVs6sX"
      },
      "outputs": [],
      "source": [
        "def fix_userhist(args, user_history_dict):\n",
        "    for user in user_history_dict:\n",
        "        n_history = len(user_history_dict[user])\n",
        "        replace = n_history < args.n_memory\n",
        "        sampled_indices = np.random.choice(\n",
        "            n_history, size=args.n_memory, replace=replace)\n",
        "        user_history_dict[user] = [user_history_dict[user][i]\n",
        "                                   for i in sampled_indices]\n",
        "    return user_history_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5XNbsFSs6pp"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "def depth_search(args, kg_entity, entity, adj_sp, number):\n",
        "    adj_entity = []\n",
        "    adj_entity.append(entity)\n",
        "    for i in range(15):  # 2 5 10 15 20 25\n",
        "        temp = entity\n",
        "        for j in range(8):  # 4 8 12 16 20 24\n",
        "            neighbors = np.array(kg_entity[temp])\n",
        "            # neighbors = np.random.choice(neighbors, size=)\n",
        "            if j == 0:\n",
        "                probly = np.ones(len(neighbors)) * (1 / len(neighbors))\n",
        "            else:\n",
        "                probly = np.ones(len(neighbors)) * 0.8\n",
        "                index = adj_sp[adj_entity[-2], neighbors].nonzero()[1]\n",
        "                probly[index] = 0.2\n",
        "            probly = probly / np.sum(probly)\n",
        "            pick_nei = np.random.choice(neighbors, size=1, p=list(probly))[0]\n",
        "            adj_entity.append(pick_nei)\n",
        "            temp = adj_entity[-1]\n",
        "    adj_entity = adj_entity[1:]\n",
        "    a = Counter(adj_entity)\n",
        "    if [entity] != list(a):\n",
        "        del a[entity]\n",
        "    b = a.most_common(number)\n",
        "    adj_entity = np.array([i[0] for i in b])\n",
        "    leng = len(adj_entity)\n",
        "    if leng < number:\n",
        "        sampled_indices = np.random.choice(\n",
        "            list(range(leng)), size=number - leng, replace=True)\n",
        "        adj_entity = np.hstack((adj_entity, adj_entity[sampled_indices]))\n",
        "    return np.array(adj_entity)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EniYKVns6mx",
        "outputId": "bbc2b9bf-24b0-41c1-d5df-ed5787c11cb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading rating file ...\n",
            "n_user=1872, n_item=3846\n",
            "n_interaction=21167\n",
            "Namespace(dataset='final', dim=64, l2_weight_rs=5e-05, lr_rs=0.01, batch_size=512, n_epochs=50, n_memory=16, use_cuda=True, n_neighbor=4, kg_weight=0.0001, dropout=0.3, f='/root/.local/share/jupyter/runtime/kernel-8de8ed51-cc19-43ef-a811-490f62c0495d.json')\n",
            "3846\n",
            "reading KG file ...\n",
            "constructing knowledge graph...\n",
            "constructing local neighbor entities of items...\n",
            "constructing global neighbor entities of items...\n",
            "number of entity  9366\n",
            "number of relations 60\n",
            "number of triples 15518\n",
            "Loaded data\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import random\n",
        "import torch as t\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--dataset', type=str, default='final',help='which dataset to use')\n",
        "parser.add_argument('--dim', type=int, default=64,help='dimension of entity and relation embeddings')\n",
        "parser.add_argument('--l2_weight_rs', type=float, default=0.00005,help='weight of the l2 regularization term')\n",
        "parser.add_argument('--lr_rs', type=float,default=0.01, help='learning rate')\n",
        "parser.add_argument('--batch_size', type=int, default=512, help='batch size')\n",
        "parser.add_argument('--n_epochs', type=int, default=50,help='the number of epochs')\n",
        "parser.add_argument('--n_memory', type=int, default=16,help='fixed size of user historical items')\n",
        "parser.add_argument('--use_cuda', type=bool, default=True,help='use cuda.')\n",
        "parser.add_argument('--n_neighbor', type=int, default=4,help='fixed size of neighbor entities')\n",
        "parser.add_argument('--kg_weight', type=float,default=0.0001, help='weight of regularization')\n",
        "parser.add_argument('--dropout', type=float, default=0.3,help='Dropout rate')\n",
        "parser.add_argument('-f')\n",
        "\n",
        "np.random.seed(2019)\n",
        "random.seed(2019)\n",
        "t.manual_seed(2019)\n",
        "t.cuda.manual_seed_all(2019)\n",
        "args = parser.parse_args()\n",
        "\n",
        "show_loss = True\n",
        "show_topk = True\n",
        "args = parser.parse_args()\n",
        "\n",
        "show_loss = True\n",
        "show_topk = True\n",
        "data_info=load_data(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MnWKve0s6j5"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CGAT(nn.Module):\n",
        "    def __init__(self, args, n_entity, n_relation, n_users, n_items):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.entity_embedding = nn.Embedding(n_entity, args.dim)\n",
        "        self.relation_embedding = nn.Embedding(\n",
        "            2 * n_relation + 1, args.dim)\n",
        "        self.user_embedding = nn.Embedding(n_users, args.dim)\n",
        "        # rs_model\n",
        "        self.linear_u_mlp = nn.Linear(3 * args.dim, args.dim)\n",
        "        self.weight_re = nn.Linear(2 * args.dim, args.dim, bias=False)\n",
        "        self.weight_agg = nn.Linear(2 * args.dim, args.dim)\n",
        "        self.gate = nn.Parameter(t.FloatTensor(args.dim))\n",
        "        self.graph_att = nn.Linear(2 * args.dim, args.dim)\n",
        "        self.item_att = nn.Linear(4 * args.dim, 1)\n",
        "        self.rnn = nn.GRU(args.dim, args.dim, batch_first=True)\n",
        "        self.trans_att = nn.Linear(args.dim, args.dim)\n",
        "        self.user_specific = True\n",
        "        if not self.user_specific:\n",
        "            self.weight_nouser = nn.Linear(args.dim, 1, bias=False)\n",
        "\n",
        "        self.Dropout = nn.Dropout(args.dropout)\n",
        "        init.zeros_(self.gate.data)\n",
        "        init.xavier_uniform_(self.relation_embedding.weight)\n",
        "\n",
        "    # (batch, n_memories, dims)\n",
        "    def forward(self, user_indices, entity_lc, relation_lc, entity_gb, kg):\n",
        "        # set_trace()\n",
        "        user_global_em = self.user_embedding(\n",
        "            user_indices)  # (batch, embeddings)  user global embedding\n",
        "        entity_hislc_em = self.entity_agg(\n",
        "            entity_lc[0], relation_lc[0], user_global_em)  # local embedding of user historical items\n",
        "        # global embedding of items (batch, n_memory, embeddings)\n",
        "        entity_hisgb_em = self.entity_agg(entity_gb[0], None, None)\n",
        "        # aggregate the local embedding and global embedding of items\n",
        "        gate = t.sigmoid(self.gate)\n",
        "        item_history_em = gate.expand_as(\n",
        "            entity_hislc_em) * entity_hislc_em + (1 - gate).expand_as(entity_hisgb_em) * entity_hisgb_em\n",
        "        item_history_em = t.cat((self.self_vectors, item_history_em), -1)\n",
        "\n",
        "        if len(item_history_em) == 1:\n",
        "            item_history_em = item_history_em.expand(\n",
        "                len(entity_lc[1][0]), item_history_em.shape[1], item_history_em.shape[2])\n",
        "            user_global_em = user_global_em.expand(\n",
        "                len(entity_lc[1][0]), user_global_em.shape[1])\n",
        "        pos_enlc_em = self.aggregate_lc(\n",
        "            entity_lc[1], relation_lc[1], user_global_em)  # (batch, dims)  local embedding of candidate items\n",
        "        pos_engb_em = self.aggregate_gb(entity_gb[1])\n",
        "        pos_item_em = gate.expand_as(pos_enlc_em) * pos_enlc_em + (\n",
        "            1 - gate).expand_as(pos_engb_em) * pos_engb_em\n",
        "        pos_item_em = t.cat((self.self_vectors, pos_item_em), -1)\n",
        "        userp_lc_em = self.rs_attention(\n",
        "            item_history_em, pos_item_em)  # (batch, dims)\n",
        "        userp_em = t.cat(\n",
        "            (user_global_em, userp_lc_em), 1)  # (batch, 2*dims)\n",
        "        userp_em = t.relu(\n",
        "            self.linear_u_mlp(self.Dropout(userp_em)))\n",
        "        userp_em = self.Dropout(userp_em)\n",
        "        userp_em = F.normalize(userp_em, dim=-1)\n",
        "        userp_em = t.cat((user_global_em, userp_em),\n",
        "                         1)  # (batch, 2 * dims)\n",
        "        pos_score = t.sum(userp_em * pos_item_em, 1)  # (batch,)\n",
        "\n",
        "        if entity_lc[2] is not None:\n",
        "            neg_enlc_em = self.aggregate_lc(\n",
        "                entity_lc[2], relation_lc[2], user_global_em)  # (batch, dims)\n",
        "            neg_engb_em = self.aggregate_gb(entity_gb[2])\n",
        "            neg_item_em = gate.expand_as(neg_enlc_em) * neg_enlc_em + (\n",
        "                1 - gate).expand_as(neg_engb_em) * neg_engb_em\n",
        "            neg_item_em = t.cat((self.self_vectors, neg_item_em), -1)\n",
        "            usern_lc_em = self.rs_attention(\n",
        "                item_history_em, neg_item_em)  # (batch, dims)\n",
        "            usern_em = t.cat(\n",
        "                (user_global_em, usern_lc_em), 1)  # (batch, 2*dims)\n",
        "            usern_em = t.relu(\n",
        "                self.linear_u_mlp(self.Dropout(usern_em)))  # (batch, dims)\n",
        "            usern_em = self.Dropout(usern_em)\n",
        "            usern_em = F.normalize(usern_em, dim=1)\n",
        "            usern_em = t.cat((user_global_em, usern_em), 1)\n",
        "            neg_score = t.sum(usern_em * neg_item_em, 1)  # (batch,)\n",
        "        else:\n",
        "            neg_score = t.zeros(1).cuda()\n",
        "        rs_loss = -t.mean(t.log(t.sigmoid(pos_score - neg_score)))\n",
        "\n",
        "        # knowledge graph loss for regularization\n",
        "        if kg[0] is not None:\n",
        "            head_em = self.entity_embedding(kg[0])\n",
        "            relation_em = self.relation_embedding(kg[1])\n",
        "            tail_em = self.entity_embedding(kg[2])\n",
        "            tail_ne_em = self.entity_embedding(kg[3])\n",
        "\n",
        "            nere_vectors = t.cat((tail_em, relation_em), 1)\n",
        "            nere_ne_vectors = t.cat(\n",
        "                (tail_ne_em, relation_em), 1)  # (batch, 2dims)\n",
        "            # (batch, dims)\n",
        "            nere_vectors = self.weight_re(nere_vectors)\n",
        "            nere_ne_vectors = self.weight_re(nere_ne_vectors)\n",
        "\n",
        "            score_kge = t.sum((head_em - nere_vectors).pow(2), 1)\n",
        "            score_kge_ne = t.sum((head_em - nere_ne_vectors).pow(2), 1)\n",
        "            kg_loss = -t.mean(t.log(t.sigmoid(score_kge_ne - score_kge)))\n",
        "        else:\n",
        "            kg_loss = t.zeros(1).cuda()\n",
        "        all_loss = rs_loss + self.args.kg_weight * kg_loss\n",
        "\n",
        "        return pos_score, neg_score, all_loss\n",
        "\n",
        "    # aggregate local embeddings of items\n",
        "    def entity_agg(self, entity_his, relation_his, user_global_em):\n",
        "        batch = entity_his[0].shape[0]\n",
        "        # (batch * n_memories, 1), (batch * n_memories, neighbor)\n",
        "        entity_his = [i.reshape([-1, i.shape[-1]]) for i in entity_his]\n",
        "        if relation_his is not None:\n",
        "            relation_his = relation_his.reshape(\n",
        "                [-1, self.args.n_neighbor])  # (batch*n_memories, neighbor)\n",
        "            user_em_att = user_global_em.unsqueeze(1).expand(\n",
        "                batch, self.args.n_memory, self.args.dim)  # (batch, n_memories, dims)\n",
        "            # (batch*n_memories, dims)\n",
        "            user_em_att = user_em_att.contiguous().view(-1, self.args.dim)\n",
        "            entity_his_vec = self.aggregate_lc(\n",
        "                entity_his, relation_his, user_em_att)  # (batch*n_memories, dims)\n",
        "            self.self_vectors = self.self_vectors.contiguous().view(\n",
        "                [batch, self.args.n_memory, self.args.dim])\n",
        "        else:\n",
        "            entity_his_vec = self.aggregate_gb(entity_his)\n",
        "        entity_his_vec = entity_his_vec.contiguous().view(\n",
        "            [batch, self.args.n_memory, self.args.dim])  # (batch, n_memories, dims)\n",
        "        return entity_his_vec\n",
        "\n",
        "    def aggregate_lc(self, entities, relations, users_embedding):\n",
        "        # (batch, 1, dims), (batch, neigh, dims)...\n",
        "        entity_vectors = [self.entity_embedding(i) for i in entities]\n",
        "        # (batch, neigh, dims)\n",
        "        relation_vectors = self.relation_embedding(relations)\n",
        "        self.self_vectors = entity_vectors[0].squeeze(1)  # (batch, dims)\n",
        "        # (batch, neigh, dims)\n",
        "        neighbor_vectors = entity_vectors[1]\n",
        "        # (batch, neigh, 2dims)\n",
        "        nere_vectors = t.cat((neighbor_vectors, relation_vectors), 2)\n",
        "        # (batch, neigh, dims)\n",
        "        nere_vectors = self.weight_re(nere_vectors)\n",
        "        user_att = t.relu(self.trans_att(users_embedding))\n",
        "        vector = self.SumAttention(self_vectors=self.self_vectors, neres=nere_vectors,\n",
        "                                   user_embeddings=user_att, neighbors=neighbor_vectors, user_specific=self.user_specific)\n",
        "        vector = self.Dropout(vector)\n",
        "        vector = F.normalize(vector)\n",
        "        return vector\n",
        "\n",
        "    def aggregate_gb(self, entities):\n",
        "        # (batch, 1, dims), (batch, neigh, dims)\n",
        "        entity_vectors = [self.entity_embedding(i) for i in entities]\n",
        "        self_vectors = entity_vectors[0].squeeze(1)  # (batch, dims)\n",
        "        # (batch, -1, neigh, dims)\n",
        "        neighbor_vectors = entity_vectors[1]  # (batch, neigh, dims)\n",
        "        output, h0 = self.rnn(neighbor_vectors)\n",
        "        # (batch, -1, dims)\n",
        "        agg_vectors = output[:, -1, :]  # (batch, dims)\n",
        "        agg_vectors = t.tanh(self.weight_agg(\n",
        "            t.cat((self_vectors, agg_vectors), 1)))  # (batch, dims)\n",
        "        agg_vectors = self.Dropout(agg_vectors)\n",
        "        agg_vectors = F.normalize(agg_vectors)\n",
        "        return agg_vectors\n",
        "\n",
        "    # aggregate the item embeddings to obation user local preference\n",
        "    def rs_attention(self, item_history_embedding, item_pre_embedding):\n",
        "        # (batch, n_memories)\n",
        "        item_pre_embedding = item_pre_embedding.unsqueeze(\n",
        "            1).expand_as(item_history_embedding)\n",
        "        logits = t.tanh(self.item_att(\n",
        "            t.cat((item_history_embedding, item_pre_embedding), 2))).squeeze(2)\n",
        "        attention = t.softmax(logits, 1)\n",
        "        user_embedding = t.matmul(attention.unsqueeze(\n",
        "            1), item_history_embedding).squeeze(1)  # (batch, dims)\n",
        "        return user_embedding\n",
        "\n",
        "    # user-specific GAT\n",
        "    def SumAttention(self, self_vectors, neres, user_embeddings, neighbors, user_specific):\n",
        "        # batch = len(self_vectors)\n",
        "        user_embeddings = user_embeddings.unsqueeze(1)  # (batch, 1, dims)\n",
        "        # (batch, -1, neighbor+1, dim)\n",
        "        if user_specific:\n",
        "            user_embeddings = user_embeddings.expand_as(\n",
        "                neighbors)  # (batch, neighbor, dims)\n",
        "            self_vectors1 = self_vectors.unsqueeze(1).expand_as(neighbors)\n",
        "            # (batch, neighbor, 2dims)\n",
        "            cat_vectors = t.cat((self_vectors1, neres), 2)\n",
        "            trans_vectors = t.tanh(self.graph_att(cat_vectors))\n",
        "            logits = t.sum(user_embeddings * trans_vectors,\n",
        "                           2)  # (batch, neighbor)\n",
        "            attention = t.softmax(logits, 1)  # (batch, neighbor)\n",
        "            new_vector = t.matmul(attention.unsqueeze(\n",
        "                1), neighbors).squeeze(1)  # (batch, dim)\n",
        "        else:\n",
        "            self_vectors1 = self_vectors.unsqueeze(1).expand_as(neighbors)\n",
        "            cat_vectors = t.cat((self_vectors1, neres), 2)\n",
        "            trans_vectors = t.tanh(self.graph_att(cat_vectors))\n",
        "            logits = self.weight_nouser(trans_vectors).squeeze(-1)\n",
        "            attention = t.softmax(logits, 1)\n",
        "            new_vector = t.matmul(attention.unsqueeze(\n",
        "                1), neighbors).squeeze(1)  # (batch, dim)\n",
        "        new_vector = t.tanh(self.weight_agg(\n",
        "            t.cat((self_vectors, new_vector), 1)))  # (batch, dims)\n",
        "        return new_vector\n",
        "    def predict(self, user_indices, item_indices):\n",
        "        user_global_em = self.user_embedding(user_indices)\n",
        "        item_em = self.entity_embedding(item_indices)\n",
        "        kg_score = self.kg_score(user_global_em, item_em)\n",
        "        return kg_score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0Fj8Jcrs6hC"
      },
      "outputs": [],
      "source": [
        "def train(args, data_info, show_loss, show_topk):\n",
        "    print('starting train...')\n",
        "    train_data = data_info[0]\n",
        "    eval_data = data_info[1]\n",
        "    test_data = data_info[2]\n",
        "    n_entity = data_info[3]\n",
        "    n_relation = data_info[4]\n",
        "    n_users = data_info[5]\n",
        "    n_items = data_info[6]\n",
        "    adj_enre = [data_info[8], data_info[9], data_info[11]]\n",
        "    kg = data_info[7]\n",
        "    user_history = data_info[10]\n",
        "    weights_save_path = '../../result_w/' + args.dataset + '/cgat/'\n",
        "\n",
        "    ensureDir(weights_save_path)\n",
        "\n",
        "    model = CGAT(args, n_entity, n_relation, n_users, n_items)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr_rs,\n",
        "                           weight_decay=args.l2_weight_rs)\n",
        "    user_list, train_record, test_record, item_set, k_list = topk_settings(\n",
        "        show_topk, train_data, test_data, n_items)\n",
        "    stopping_step = 0\n",
        "    should_stop = False\n",
        "    cur_best_pre_0 = 0\n",
        "    if args.use_cuda:\n",
        "        model.cuda()\n",
        "    for epoch in range(args.n_epochs):\n",
        "        n = 0\n",
        "        model.train()\n",
        "        np.random.shuffle(train_data)\n",
        "        np.random.shuffle(kg)\n",
        "        t1 = time.time()\n",
        "        loss_r = t.zeros(1)\n",
        "        for user_indices, entity_lc, relation_lc, entity_gb, kg_indice in minibatch_rs(args, train_data, user_history, kg, adj_enre, True):\n",
        "            user_indices = t.LongTensor(user_indices).cuda()\n",
        "            entity_lc_cuda = []\n",
        "            relation_lc_cuda = []\n",
        "            entity_gb_cuda = []\n",
        "            kg_cuda = []\n",
        "            for i in range(len(entity_lc)):\n",
        "                entity_lc_cuda.append([t.LongTensor(entity).cuda()\n",
        "                                       for entity in entity_lc[i]])\n",
        "                relation_lc_cuda.append(t.LongTensor(relation_lc[i]).cuda())\n",
        "                entity_gb_cuda.append([t.LongTensor(entity).cuda()\n",
        "                                       for entity in entity_gb[i]])\n",
        "            for i in range(len(kg_indice)):\n",
        "                kg_cuda.append(t.LongTensor(kg_indice[i]).cuda())\n",
        "\n",
        "            score1, score2, all_loss = model(\n",
        "                user_indices, entity_lc_cuda, relation_lc_cuda, entity_gb_cuda, kg_cuda)\n",
        "            optimizer.zero_grad()\n",
        "            all_loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_r += (all_loss.cpu())\n",
        "            n += 1\n",
        "        t2 = time.time()\n",
        "        if show_loss:\n",
        "            print('time: {:.4f}'.format(t2 - t1))\n",
        "            print('ave_loss: %.4f' % (loss_r.data.cpu().numpy() / n))\n",
        "        # result used to tune hyperparameters\n",
        "        # eval_auc = evaluation(\n",
        "        #     model, eval_data, user_history, kg, adj_enre, args)\n",
        "        # test_auc = evaluation(\n",
        "        #     model, test_data, user_history, kg, adj_enre, args)\n",
        "        # print('epoch %d    eval auc: %.4f  test auc: %.4f'\n",
        "        #       % (epoch + 1, eval_auc, test_auc))\n",
        "\n",
        "        # Top evaluation\n",
        "        if show_topk and (epoch + 1) % 5 == 0:\n",
        "            precision, recall, ndcg, hit, result, ave_time = topk_eval(\n",
        "                args, model, user_list, user_history, train_record, test_record, item_set, k_list, adj_enre)\n",
        "            print('precision: ', end='')\n",
        "            for i in precision:\n",
        "                print('%.4f\\t' % i, end='')\n",
        "            print()\n",
        "            print('recall: ', end='')\n",
        "            for i in recall:\n",
        "                print('%.4f\\t' % i, end='')\n",
        "            print()\n",
        "            print('ndcg: ', end='')\n",
        "            for i in ndcg:\n",
        "                print('%.4f\\t' % i, end='')\n",
        "            print()\n",
        "            print('hit: ', end='')\n",
        "            for i in hit:\n",
        "                print('%.4f\\t' % i, end='')\n",
        "            print('\\n')\n",
        "            # print('averaged time of predicting for each user:', ave_time)\n",
        "            cur_best_pre_0, stopping_step, should_stop = early_stopping(recall[1], cur_best_pre_0,\n",
        "                                                                        stopping_step, expected_order='acc', flag_step=2)\n",
        "            if should_stop is True:\n",
        "                break\n",
        "            # *********************************************************\n",
        "            # save the user & item embeddings for pretraining.\n",
        "            # if recall[3] == cur_best_pre_0:\n",
        "                # t.save(model.state_dict(), weights_save_path + 'weights.pth')\n",
        "                # pickle.dump(result, open(weights_save_path + 'results', 'wb'))\n",
        "                # print('save the weights in path: ', weights_save_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCP8xzj6s6eS"
      },
      "outputs": [],
      "source": [
        "def ensureDir(dir_path):\n",
        "    d = os.path.dirname(dir_path)\n",
        "    if not os.path.exists(d):\n",
        "        os.makedirs(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMBgjtFJs6b1"
      },
      "outputs": [],
      "source": [
        "def minibatch_rs(args, data, user_history, kg, adj_enre, train=True):\n",
        "    adj_enlc, adj_relc, adj_engb = adj_enre[0], adj_enre[1], adj_enre[2]\n",
        "    data_size = len(data)\n",
        "    l1 = int(data_size / args.batch_size) + 1\n",
        "    for i in range(l1):\n",
        "        start = args.batch_size * i\n",
        "        end = min(args.batch_size * (i + 1), data_size)\n",
        "        user_indices = data[start:end, 0]  # (batch,)\n",
        "        pos_item_indices = data[start:end, 1]  # (batch,)\n",
        "        neg_item_indices = data[start:end, 2]\n",
        "\n",
        "        user_his_batch = np.array(\n",
        "            [user_history[user] for user in user_indices])  # (batch, n_memory)\n",
        "        entity_his_lc, relation_his_lc = get_neighbors(\n",
        "            args, np.reshape(user_his_batch, -1), adj_enlc, adj_relc)\n",
        "        entity_his_gb = get_neighbors(\n",
        "            args, np.reshape(user_his_batch, -1), adj_engb, None)\n",
        "\n",
        "        # (batch, n_memory, 1), (batch, n_memory, n_neigh) the neighbors of user historical items\n",
        "        entity_his_lc = [np.reshape(\n",
        "            entity, [len(user_indices), args.n_memory, -1]) for entity in entity_his_lc]\n",
        "        relation_his_lc = np.reshape(\n",
        "            relation_his_lc, [len(user_indices), args.n_memory, args.n_neighbor])\n",
        "        entity_his_gb = [np.reshape(\n",
        "            entity, [len(user_indices), args.n_memory, -1]) for entity in entity_his_gb]\n",
        "\n",
        "        # [(batch, 1),(batch, n_neigh)]; (batch, n_neigh)  the neighbor of candidate items\n",
        "        pos_entity_lc, pos_relation_lc = get_neighbors(\n",
        "            args, pos_item_indices, adj_enlc, adj_relc)\n",
        "        neg_entity_lc, neg_relation_lc = get_neighbors(\n",
        "            args, neg_item_indices, adj_enlc, adj_relc)\n",
        "        pos_entity_gb = get_neighbors(\n",
        "            args, pos_item_indices, adj_engb, None)\n",
        "        neg_entity_gb = get_neighbors(\n",
        "            args, neg_item_indices, adj_engb, None)\n",
        "\n",
        "        entity_lc = (entity_his_lc, pos_entity_lc, neg_entity_lc)\n",
        "        relation_lc = (relation_his_lc, pos_relation_lc, neg_relation_lc)\n",
        "        entity_gb = (entity_his_gb, pos_entity_gb, neg_entity_gb)\n",
        "        if train:\n",
        "            if end > len(kg):\n",
        "                start = len(kg) - args.batch_size\n",
        "                end = len(kg)\n",
        "            kg_de = kg[start:end]\n",
        "            head_indices = kg_de[:, 0]\n",
        "            relation_indices = kg_de[:, 1]\n",
        "            tail_indices = kg_de[:, 2]\n",
        "            tail_indices_ne = kg_de[:, 3]\n",
        "        else:\n",
        "            head_indices, relation_indices = None, None\n",
        "            tail_indices, tail_indices_ne = None, None\n",
        "        kg_indice = (head_indices, relation_indices,\n",
        "                     tail_indices, tail_indices_ne)\n",
        "        yield user_indices, entity_lc, relation_lc, entity_gb, kg_indice\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGVqiKads6ZT"
      },
      "outputs": [],
      "source": [
        "def evaluation(model, data, user_history, kg, adj_enre, args):\n",
        "    model.eval()\n",
        "    auc_list = []\n",
        "    for user_indices, entity_lc, relation_lc, entity_gb, kg_indice in minibatch_rs(args, data, user_history, kg, adj_enre, False):\n",
        "        user_indices = t.LongTensor(user_indices).cuda()\n",
        "        entity_lc_cuda = []\n",
        "        relation_lc_cuda = []\n",
        "        entity_gb_cuda = []\n",
        "        # kg_cuda = []\n",
        "        for i in range(len(entity_lc)):\n",
        "            entity_lc_cuda.append([t.LongTensor(entity).cuda()\n",
        "                                   for entity in entity_lc[i]])\n",
        "            relation_lc_cuda.append(t.LongTensor(relation_lc[i]).cuda())\n",
        "            entity_gb_cuda.append([t.LongTensor(entity).cuda()\n",
        "                                   for entity in entity_gb[i]])\n",
        "\n",
        "        score1, score2, all_loss = model(\n",
        "            user_indices, entity_lc_cuda, relation_lc_cuda, entity_gb_cuda, kg_indice)\n",
        "        score1 = score1.data.cpu().numpy()\n",
        "        score2 = score2.data.cpu().numpy()\n",
        "        scores = np.concatenate((score1, score2))\n",
        "        label1 = np.ones(len(score1))\n",
        "        label2 = np.zeros(len(score2))\n",
        "        labels = np.concatenate((label1, label2))\n",
        "        auc = roc_auc_score(y_true=labels, y_score=scores)\n",
        "        auc_list.append(auc)\n",
        "    model.train()\n",
        "    return float(np.mean(auc_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bzx9MFqRs6WX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_user_record(data):\n",
        "    user_history_dict = dict()\n",
        "    for interaction in data:\n",
        "        user = interaction[0]\n",
        "        item = interaction[1]\n",
        "        if user not in user_history_dict:\n",
        "            user_history_dict[user] = set()\n",
        "        user_history_dict[user].add(item)\n",
        "    return user_history_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns77F8ews6Tq"
      },
      "outputs": [],
      "source": [
        "def topk_settings(show_topk, train_data, test_data, n_item):\n",
        "    if show_topk:\n",
        "        # user_num = 100\n",
        "        k_list = [10, 20, 50]\n",
        "        train_record = get_user_record(train_data)\n",
        "        test_record = get_user_record(test_data)\n",
        "        user_list = list(set(train_record.keys()) & set(test_record.keys()))\n",
        "        # if len(user_list) > user_num:\n",
        "        #     user_list = np.random.choice(\n",
        "        #         user_list, size=user_num, replace=False)\n",
        "        item_set = set(list(range(n_item)))\n",
        "        return user_list, train_record, test_record, item_set, k_list\n",
        "    else:\n",
        "        return [None] * 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMR9sJCHs6Qy"
      },
      "outputs": [],
      "source": [
        "def topk_eval(args, model, user_list, user_history, train_record, test_record, item_set, k_list, adj_enre):\n",
        "    precision_list = {k: [] for k in k_list}\n",
        "    recall_list = {k: [] for k in k_list}\n",
        "    ndcg_list = {k: [] for k in k_list}\n",
        "    hit_list = {k: [] for k in k_list}\n",
        "    result = dict()\n",
        "    adj_enlc, adj_relc, adj_engb = adj_enre[0], adj_enre[1], adj_enre[2]\n",
        "    all_time = 0\n",
        "    for user in user_list:\n",
        "        # time0 = time.time()\n",
        "        test_item_list = list(item_set - set(train_record[user]))\n",
        "        item_score_map = dict()\n",
        "        start = 0\n",
        "        while start < len(test_item_list):\n",
        "            end = min(start + args.batch_size, len(test_item_list))\n",
        "            item_indices = test_item_list[start:end]\n",
        "            # user_indices = [user] * (end - start)\n",
        "            user_indices = [user]\n",
        "            user_his_batch = np.array(\n",
        "                [user_history[user] for user in user_indices])  # (batch, n_memory)\n",
        "            entity_his_lc, relation_his_lc = get_neighbors(\n",
        "                args, np.reshape(user_his_batch, -1), adj_enlc, adj_relc)\n",
        "            entity_his_gb = get_neighbors(\n",
        "                args, np.reshape(user_his_batch, -1), adj_engb, None)\n",
        "\n",
        "            # (batch, n_memory, 1), (batch, n_memory, n_neigh).\n",
        "            entity_his_lc = [np.reshape(\n",
        "                entity, [len(user_indices), args.n_memory, -1]) for entity in entity_his_lc]\n",
        "            relation_his_lc = np.reshape(\n",
        "                relation_his_lc, [len(user_indices), args.n_memory, args.n_neighbor])\n",
        "            entity_his_gb = [np.reshape(\n",
        "                entity, [len(user_indices), args.n_memory, -1]) for entity in entity_his_gb]\n",
        "            # (batch, 1),(batch, n_neigh)\n",
        "            pos_entity_lc, pos_relation_lc = get_neighbors(\n",
        "                args, item_indices, adj_enlc, adj_relc)\n",
        "            pos_entity_gb = get_neighbors(\n",
        "                args, item_indices, adj_engb, None)\n",
        "\n",
        "            entity_his_lc = [t.LongTensor(\n",
        "                i).cuda() for i in entity_his_lc]\n",
        "            pos_entity_lc = [t.LongTensor(i).cuda() for i in pos_entity_lc]\n",
        "            relation_his_lc = t.LongTensor(relation_his_lc).cuda()\n",
        "            pos_relation_lc = t.LongTensor(pos_relation_lc).cuda()\n",
        "            user_indices = t.LongTensor(user_indices).cuda()\n",
        "            entity_his_gb = [t.LongTensor(\n",
        "                i).cuda() for i in entity_his_gb]\n",
        "            pos_entity_gb = [t.LongTensor(i).cuda() for i in pos_entity_gb]\n",
        "\n",
        "            neg_entity_lc = None\n",
        "            neg_relation_lc = None\n",
        "            neg_entity_gb = None, None\n",
        "            head_indices, relation_indices = None, None\n",
        "            tail_indices, tail_indices_ne = None, None\n",
        "            entity_lc_cuda = [entity_his_lc, pos_entity_lc, neg_entity_lc]\n",
        "            relation_lc_cuda = [relation_his_lc,\n",
        "                                pos_relation_lc, neg_relation_lc]\n",
        "            entity_gb_cuda = [entity_his_gb, pos_entity_gb, neg_entity_gb]\n",
        "            kg_cuda = [head_indices, relation_indices,\n",
        "                       tail_indices, tail_indices_ne]\n",
        "            time1 = time.time()\n",
        "            score1, score2, all_loss = model(\n",
        "                user_indices, entity_lc_cuda, relation_lc_cuda, entity_gb_cuda, kg_cuda)\n",
        "            all_time += (time.time() - time1)\n",
        "            for item, score in zip(item_indices, list(score1.data.cpu().numpy())):\n",
        "                item_score_map[item] = score\n",
        "            start += args.batch_size\n",
        "        item_score_pair_sorted = sorted(\n",
        "            item_score_map.items(), key=lambda x: x[1], reverse=True)\n",
        "        item_sorted = [i[0] for i in item_score_pair_sorted]\n",
        "        label = np.zeros(len(item_sorted))\n",
        "        for i, item in enumerate(item_sorted):\n",
        "            if item in test_record[user]:\n",
        "                label[i] = 1\n",
        "        for k in k_list:\n",
        "            hit_num = np.sum(label[:k])\n",
        "            precision_list[k].append(hit_num / k)\n",
        "            recall_list[k].append(hit_num / len(test_record[user]))\n",
        "            ndcg_list[k].append(comp_ndcg(label, k))\n",
        "            if np.sum(label[:k]) > 0:\n",
        "                hit_list[k].append(1)\n",
        "            else:\n",
        "                hit_list[k].append(0)\n",
        "    result['precision'] = np.array([precision_list[k] for k in k_list])\n",
        "    result['recall'] = np.array([recall_list[k] for k in k_list])\n",
        "    result['ndcg'] = np.array([ndcg_list[k] for k in k_list])\n",
        "    result['hit'] = np.array([hit_list[k] for k in k_list])\n",
        "    precision = [np.mean(precision_list[k]) for k in k_list]\n",
        "    recall = [np.mean(recall_list[k]) for k in k_list]\n",
        "    ndcg = [np.mean(ndcg_list[k]) for k in k_list]\n",
        "    hit = [np.mean(hit_list[k]) for k in k_list]\n",
        "    model.train()\n",
        "    return precision, recall, ndcg, hit, result, all_time / len(user_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOrQ1hzbs6N0"
      },
      "outputs": [],
      "source": [
        "def comp_ndcg(label, k):\n",
        "    topk = label[:k]\n",
        "    dcg = np.sum(topk / np.log2(np.arange(2, topk.size + 2)))\n",
        "    dcg_max = np.sum(sorted(label, reverse=True)[\n",
        "                     :k] / np.log2(np.arange(2, topk.size + 2)))\n",
        "    if dcg_max == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return dcg / dcg_max\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1z_hxTQs6Kj"
      },
      "outputs": [],
      "source": [
        "def get_neighbors(args, seeds, adj_entity, adj_relation):\n",
        "    seeds_size = len(seeds)  # (batch, 1)\n",
        "    seeds = np.expand_dims(seeds, axis=1)\n",
        "    entities = [seeds]\n",
        "    if adj_relation is not None:\n",
        "        neighbor_entities = np.reshape(\n",
        "            adj_entity[np.reshape(entities[0], -1)], [seeds_size, -1])\n",
        "        neighbor_relations = np.reshape(\n",
        "            adj_relation[np.reshape(entities[0], -1)], [seeds_size, -1])  # (batch, neighbor)\n",
        "        entities.append(neighbor_entities)\n",
        "        # relations.append(neighbor_relations)\n",
        "        return entities, neighbor_relations\n",
        "    else:\n",
        "        neighbor_entities = np.reshape(\n",
        "            adj_entity[np.reshape(entities[0], -1)], [seeds_size, -1])\n",
        "        entities.append(neighbor_entities)\n",
        "        return entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfcklr7ts6H8"
      },
      "outputs": [],
      "source": [
        "def early_stopping(log_value, best_value, stopping_step, expected_order='acc', flag_step=2):\n",
        "    # early stopping strategy:\n",
        "    assert expected_order in ['acc', 'dec']\n",
        "\n",
        "    if (expected_order == 'acc' and log_value >= best_value) or (expected_order == 'dec' and log_value <= best_value):\n",
        "        stopping_step = 0\n",
        "        best_value = log_value\n",
        "    else:\n",
        "        stopping_step += 1\n",
        "\n",
        "    if stopping_step >= flag_step:\n",
        "        print(\"Early stopping is trigger at step: {} log:{}\".format(\n",
        "            flag_step, log_value))\n",
        "        should_stop = True\n",
        "    else:\n",
        "        should_stop = False\n",
        "    return best_value, stopping_step, should_stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQdWeKyrReDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7c3c907-e25e-49cf-b6f8-8eb189c3d9c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting train...\n",
            "time: 5.0637\n",
            "ave_loss: 2.5789\n",
            "time: 2.6254\n",
            "ave_loss: 0.4430\n",
            "time: 2.6519\n",
            "ave_loss: 0.0856\n",
            "time: 3.0275\n",
            "ave_loss: 0.0442\n",
            "time: 3.0392\n",
            "ave_loss: 0.0490\n",
            "precision: 0.0070\t0.0057\t0.0046\t\n",
            "recall: 0.0284\t0.0474\t0.0936\t\n",
            "ndcg: 0.0178\t0.0239\t0.0356\t\n",
            "hit: 0.0678\t0.1084\t0.2059\t\n",
            "\n",
            "time: 2.8010\n",
            "ave_loss: 0.0613\n",
            "time: 3.0526\n",
            "ave_loss: 0.0748\n",
            "time: 3.0559\n",
            "ave_loss: 0.0879\n",
            "time: 2.6647\n",
            "ave_loss: 0.1000\n",
            "time: 2.6451\n",
            "ave_loss: 0.1098\n",
            "precision: 0.0158\t0.0127\t0.0089\t\n",
            "recall: 0.0656\t0.1040\t0.1812\t\n",
            "ndcg: 0.0428\t0.0554\t0.0748\t\n",
            "hit: 0.1448\t0.2180\t0.3495\t\n",
            "\n",
            "time: 3.0603\n",
            "ave_loss: 0.1154\n",
            "time: 2.7636\n",
            "ave_loss: 0.1201\n",
            "time: 2.7512\n",
            "ave_loss: 0.1229\n",
            "time: 2.8946\n",
            "ave_loss: 0.1246\n",
            "time: 3.3173\n",
            "ave_loss: 0.1252\n",
            "precision: 0.0370\t0.0280\t0.0176\t\n",
            "recall: 0.1552\t0.2285\t0.3601\t\n",
            "ndcg: 0.1056\t0.1299\t0.1631\t\n",
            "hit: 0.2998\t0.4161\t0.5712\t\n",
            "\n",
            "time: 2.7908\n",
            "ave_loss: 0.1246\n",
            "time: 3.0627\n",
            "ave_loss: 0.1243\n",
            "time: 3.0581\n",
            "ave_loss: 0.1236\n",
            "time: 2.6937\n",
            "ave_loss: 0.1235\n",
            "time: 2.6798\n",
            "ave_loss: 0.1232\n",
            "precision: 0.0481\t0.0352\t0.0205\t\n",
            "recall: 0.1972\t0.2867\t0.4197\t\n",
            "ndcg: 0.1406\t0.1695\t0.2032\t\n",
            "hit: 0.3780\t0.4930\t0.6426\t\n",
            "\n",
            "time: 2.8406\n",
            "ave_loss: 0.1228\n",
            "time: 2.6621\n",
            "ave_loss: 0.1229\n",
            "time: 2.6261\n",
            "ave_loss: 0.1224\n",
            "time: 2.8357\n",
            "ave_loss: 0.1222\n",
            "time: 3.3090\n",
            "ave_loss: 0.1223\n",
            "precision: 0.0497\t0.0364\t0.0215\t\n",
            "recall: 0.2049\t0.2971\t0.4373\t\n",
            "ndcg: 0.1434\t0.1733\t0.2092\t\n",
            "hit: 0.3846\t0.5064\t0.6632\t\n",
            "\n",
            "time: 2.8051\n",
            "ave_loss: 0.1227\n",
            "time: 2.9072\n",
            "ave_loss: 0.1229\n",
            "time: 3.2517\n",
            "ave_loss: 0.1225\n",
            "time: 2.6886\n",
            "ave_loss: 0.1223\n",
            "time: 2.6839\n",
            "ave_loss: 0.1227\n",
            "precision: 0.0512\t0.0372\t0.0217\t\n",
            "recall: 0.2078\t0.3026\t0.4421\t\n",
            "ndcg: 0.1465\t0.1772\t0.2126\t\n",
            "hit: 0.3967\t0.5173\t0.6626\t\n",
            "\n",
            "time: 3.0509\n",
            "ave_loss: 0.1233\n",
            "time: 2.7114\n",
            "ave_loss: 0.1235\n",
            "time: 2.6935\n",
            "ave_loss: 0.1234\n",
            "time: 2.6873\n",
            "ave_loss: 0.1233\n",
            "time: 3.4272\n",
            "ave_loss: 0.1231\n",
            "precision: 0.0506\t0.0376\t0.0219\t\n",
            "recall: 0.2063\t0.3067\t0.4420\t\n",
            "ndcg: 0.1445\t0.1767\t0.2115\t\n",
            "hit: 0.3967\t0.5179\t0.6602\t\n",
            "\n",
            "time: 2.7911\n",
            "ave_loss: 0.1234\n",
            "time: 2.7349\n",
            "ave_loss: 0.1237\n",
            "time: 3.3176\n",
            "ave_loss: 0.1239\n",
            "time: 2.7221\n",
            "ave_loss: 0.1232\n",
            "time: 2.7020\n",
            "ave_loss: 0.1237\n",
            "precision: 0.0507\t0.0366\t0.0216\t\n",
            "recall: 0.2080\t0.2969\t0.4401\t\n",
            "ndcg: 0.1453\t0.1743\t0.2107\t\n",
            "hit: 0.4004\t0.5070\t0.6584\t\n",
            "\n",
            "time: 3.1375\n",
            "ave_loss: 0.1237\n",
            "time: 2.6868\n",
            "ave_loss: 0.1242\n",
            "time: 2.6487\n",
            "ave_loss: 0.1246\n",
            "time: 2.6722\n",
            "ave_loss: 0.1242\n",
            "time: 3.3712\n",
            "ave_loss: 0.1245\n",
            "precision: 0.0497\t0.0364\t0.0215\t\n",
            "recall: 0.2037\t0.2941\t0.4341\t\n",
            "ndcg: 0.1443\t0.1738\t0.2096\t\n",
            "hit: 0.3846\t0.5064\t0.6511\t\n",
            "\n",
            "Early stopping is trigger at step: 2 log:0.2941055444992453\n"
          ]
        }
      ],
      "source": [
        "model=train(args, data_info, show_loss, show_topk)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}